# -*- coding: utf-8 -*-
"""kerusiko_w2b.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MY6dNIuQSlK8_sdfmfX8aMz2Iv4hD2cU
"""

import spacy
import numpy as np

from gensim.models import Word2Vec

import time

start=time.time()

# spacyで形態素解析しつつ、ユニークな単語ボキャブラリを取ってくる
nlp = spacy.load("ja_ginza")

def tokenize(text):
  """ 形態素解析を行う """
  doc = nlp(text)
  sequence = []
  for token in doc:
    #if not token.pos_ == "AUX" or token.pos_ == "ADP" or token.pos_ == "ADV":
      sequence.append(token.text)
    
  vocab = list(np.unique(np.array(sequence)))
  return sequence, vocab

def get_corpus():
    path = "./data/corpus.txt"
    file = open(path, 'r', encoding="utf-8")

    text = file.read().replace("\n","")    

    return tokenize(text)

def tokenize_seq(text):
  """ 形態素解析を行う """
  doc = nlp(text)
  sequence = []
  for token in doc:
      sequence.append(token.text)
  return sequence

def get_corpus_2list():
    path = "./data/corpus.txt"
    file = open(path, 'r', encoding="utf-8")

    text_li=[line.replace("\n","") for line in file.readlines()]

    lis=[tokenize_seq(li) for li in text_li]

    return lis

s , vocab=get_corpus()

sequence=get_corpus_2list()
sequence

# ボキャブラリと分かち書き文章から、データセットを作成。
#word_to_id = dict((c,i) for i,c in enumerate(vocab))
#id_to_word = dict((i,c) for i,c in enumerate(vocab))
#print(word_to_id["アーミヤ"])
#print(id_to_word[166])

# 分かち書きした文章をIDで表記

#wakati_ids = []
#for word in sequence:
#  wakati_ids.append(word_to_id[word])

#print(wakati_ids[:20])
#print(sequence[:20])

window_size = 5
hidden_size = 20 # 密ベクトルのサイズ
batch_size = 100 # 一度に処理するサンプル数
max_epoch = 1000 # 重み更新回数（学習回数）


sequence

print(f"#####{len(vocab)}#####")

model = Word2Vec(sentences=sequence, size=100, window=5, min_count=1, workers=4)
model.save("./model/word2vec.model")

fin = time.time()

print(f"実行時間{int(fin-start)}")